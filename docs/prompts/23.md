Task: Integrate `aider` in Wrapper: Modify the Python wrapper to initialize and run the actual `aider` Coder.
Requirements:
1.  Modify `aider_wrapper.py` (from Prompt 12):
2.  Import necessary components from `aider`: `from aider.coders import Coder` and `from aider.io import InputOutput`.
3.  In the main execution block:
    -   Parse arguments (`--prompt`, `--context-file`).
    -   Initialize `io = InputOutput(yes=True)` for non-interactive mode.
    -   Initialize the coder: `coder = Coder(main_model=None, io=io, fnames=context_files)` (replace `context_files` with the parsed list from `--context-file`. Use default model for now). Handle potential initialization errors.
    -   Wrap the execution in a `try...except` block.
    -   Inside `try`: Run the coder: `coder.run(with_message=prompt)` (use `with_message` instead of positional).
    -   Determine success/failure based on whether an exception occurred.
    -   Construct the JSON output object (defined in spec 6.5):
        -   Set `overall_status` to "success" or "failure".
        -   Set `error` to the exception message if failure, else `null`.
        -   Populate `events` array:
            -   Initially, add a single `{"type": "status_message", "content": "Aider run completed."}` on success or `{"type": "status_message", "content": "Aider run failed."}` on failure. More detailed events later.
            -   Consider capturing `coder.io.tool_output` or similar for basic text output and add a `text_response` event if available.
    -   Print the JSON object to stdout.
    -   If an exception occurred, print the stack trace to stderr.
4.  Update `requirements.txt` if specific `aider-chat` version is needed.
Testing:
-   Modify `test/aider_wrapper.test.js`.
-   These tests become harder as they involve the real `aider` library.
-   Option 1 (Mocking `aider`): Use Python's `unittest.mock` within the test setup (e.g., via a helper script called by Node) to mock `aider.coders.Coder` and `coder.run`. Test that `Coder` is initialized with correct args (`fnames`, `io`) and `coder.run` is called with the prompt. Assert the JSON output reflects mock success/failure.
-   Option 2 (Basic Integration): Run the script against a *minimal* test case. Provide a simple prompt and maybe a dummy context file. Assert the script exits cleanly (code 0) and produces valid JSON output (structure check), without necessarily validating `aider`'s specific behavior yet. This requires `aider-chat` to be installed where tests run.
-   Ensure tests run via `npm test`. 